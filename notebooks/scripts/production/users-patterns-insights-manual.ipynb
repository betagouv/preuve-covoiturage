{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "   Parameter name          Example Value                                            Description\n",
    "- `connection_string` : 'postgresql://postgres:postgres@localhost:5432/local'   -> Postgresql URL connection string\n",
    "- `aom_insee` :          '217500016'                                            -> Aom insee code representing geo perimeter to apply the algorithm\n",
    "- `start_date` :         '2023-02-28 23:59:59'                                  -> Start date\n",
    "- `end_date`:             '2023-04-30 00:00:01'                                 -> End date\n",
    "- `policy_id`             : 459                                                 -> Policy id filter on incentive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sqlalchemy import create_engine, text\n",
    "import networkx as nx\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "load_dotenv()\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "sys.path.append('/notebooks/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aom_insee = '217500016'\n",
    "start_date ='2023-02-28 23:59:59'\n",
    "end_date='2023-04-30 00:00:01'\n",
    "policy_id = 459\n",
    "\n",
    "DATABASE = os.environ['DATABASE']\n",
    "USER = os.environ['USER_DB']\n",
    "PASSWORD = os.environ['PASSWORD']\n",
    "PORT = os.environ['PORT']\n",
    "HOST = os.environ['HOST']\n",
    "\n",
    "engine = create_engine(f\"postgresql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\",connect_args={'sslmode':'require'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "start_date = (now.replace(day=1) - datetime.timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')\n",
    "end_date = now.replace(day=1).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "query = f\"\"\"SELECT cc._id, cc.is_driver, ci.phone_trunc, cc.datetime, cc.duration, cc.operator_id, cc.seats,\n",
    "ST_AsText(cc.start_position) as start_wkt, ST_AsText(cc.end_position) as end_wkt, \n",
    "cc.operator_journey_id,\n",
    "cc.distance,\n",
    "ci.operator_user_id,\n",
    "cc.end_position,\n",
    "CASE WHEN pi.result >= 0 THEN pi.result ELSE 0 END as incentive,\n",
    "cc.operator_trip_id,\n",
    " \n",
    "cc2.is_driver as other_is_driver,\n",
    "ci2.phone_trunc as other_phone_trunc\n",
    "FROM CARPOOL.CARPOOLS cc\n",
    "   join carpool.identities ci on cc.identity_id = ci._id\n",
    "   join geo.perimeters gps on cc.start_geo_code = gps.arr and gps.year = 2022\n",
    "   join geo.perimeters gpe on cc.end_geo_code = gpe.arr and gpe.year = 2022\n",
    "   LEFT JOIN policy.incentives pi on pi.carpool_id = cc._id and pi.policy_id = '{policy_id}'\n",
    "   JOIN CARPOOL.CARPOOLS AS CC2 ON CC.OPERATOR_JOURNEY_ID = CC2.OPERATOR_JOURNEY_ID and CC.is_driver != cc2.is_driver\n",
    "   JOIN CARPOOL.IDENTITIES AS CI2 on CC2.IDENTITY_ID = CI2._id\n",
    "    WHERE CC.DATETIME >= '{start_date}'::timestamp AT TIME ZONE 'EUROPE/PARIS'\n",
    "    AND CC.DATETIME < '{end_date}'::timestamp AT TIME ZONE 'EUROPE/PARIS'\n",
    "    {f\"and (gps.aom = '{aom_insee}' or gpe.aom = '{aom_insee}' or gps.reg = '{aom_insee}' or gpe.reg = '{aom_insee}') and gps.year = 2022 and gpe.year = 2022\" if aom_insee else \"\"}\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_carpool = pd.read_sql_query(text(query), conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 1 \n",
    "Conversion des des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carpool['datetime'] = pd.to_datetime(df_carpool['datetime'])\n",
    "df_carpool['day'] = df_carpool['datetime'].dt.date\n",
    "df_carpool['incentive'] = df_carpool['incentive']/100\n",
    "df_carpool['duration'] = np.round(df_carpool['duration']/60)\n",
    "df_carpool['distance'] = np.round(df_carpool['distance']/1000,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 2\n",
    "\n",
    "Création de fonctions pour le calcul de changement de rôle par jours etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_day_change_count(row):\n",
    "    if len(row['roles']) <= 1:\n",
    "        return 0\n",
    "    count = sum((row['roles'][i] != row['roles'][i+1]) for i in range(len(row['roles']) - 1) if row['carpool_day_list'][i] == row['carpool_day_list'][i+1])\n",
    "    return count\n",
    "\n",
    "def total_change_count(row):\n",
    "    if len(row['roles']) <= 1:\n",
    "        return 0\n",
    "    count = sum((row['roles'][i] != row['roles'][i+1]) for i in range(len(row['roles']) - 1))\n",
    "    return count\n",
    "\n",
    "def intra_day_change_percentage(row):\n",
    "    unique_days = np.unique(row['carpool_day_list'])\n",
    "    count = sum(1 for day in unique_days if any((row['roles'][i] != row['roles'][i+1]) for i in range(len(row['roles']) - 1) if row['carpool_day_list'][i] == day and row['carpool_day_list'][i+1] == day))\n",
    "    percentage = np.round(count / len(unique_days) * 100, 2)\n",
    "    return percentage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 3 \n",
    "Calcul des indicateurs par phone_trunc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_trunc_insights_df = df_carpool.groupby('phone_trunc').agg({\n",
    "    'datetime': ['min', 'max'],\n",
    "    'duration':  ['mean', 'count'],\n",
    "    'distance': 'mean',\n",
    "    'incentive': 'sum',\n",
    "    'is_driver': ['mean',list],\n",
    "    'day': ['nunique', list],\n",
    "    'operator_journey_id': [list],\n",
    "    'operator_id': [list]\n",
    "})\n",
    "\n",
    "phone_trunc_insights_df.columns = ['departure_date',\n",
    "                                   'end_date',\n",
    "                                   'average_duration',\n",
    "                                   'num_trips',\n",
    "                                   'average_distance',\n",
    "                                   'total_incentives',\n",
    "                                   'driver_trip_percentage',\n",
    "                                   'roles',\n",
    "                                   'carpool_days',\n",
    "                                   'carpool_day_list',\n",
    "                                   'trip_id_list',\n",
    "                                   'operator_list']\n",
    "\n",
    "phone_trunc_insights_df.reset_index(inplace=True)\n",
    "\n",
    "phone_trunc_insights_df['num_days'] = (phone_trunc_insights_df['end_date'].dt.date - phone_trunc_insights_df['departure_date'].dt.date).dt.days\n",
    "phone_trunc_insights_df['average_trip_count'] = phone_trunc_insights_df.apply(\n",
    "    lambda row: row['num_trips'] / row['carpool_days'] if row['carpool_days'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "phone_trunc_insights_df['driver_trip_percentage'] = np.round(phone_trunc_insights_df['driver_trip_percentage'] * 100,2)\n",
    "\n",
    "\n",
    "phone_trunc_insights_df['num_operators'] = phone_trunc_insights_df['operator_list'].apply(lambda row: len(np.unique(row)))\n",
    "phone_trunc_insights_df['role_change'] = phone_trunc_insights_df['roles'].apply(lambda x: len(np.unique(x)) > 1)\n",
    "\n",
    "\n",
    "\n",
    "phone_trunc_insights_df['intraday_change_count'] = phone_trunc_insights_df.apply(intra_day_change_count, axis=1)\n",
    "phone_trunc_insights_df['total_change_count'] = phone_trunc_insights_df.apply(total_change_count, axis=1)\n",
    "\n",
    "\n",
    "phone_trunc_insights_df['intraday_change_percentage'] = phone_trunc_insights_df.apply(intra_day_change_percentage, axis=1)\n",
    "\n",
    "phone_trunc_insights_df['total_change_percentage'] = phone_trunc_insights_df.apply(lambda row: np.round(row['total_change_count'] / len(row['operator_list']) * 100, 2), axis=1)\n",
    "phone_trunc_insights_df = phone_trunc_insights_df[['phone_trunc', 'departure_date', 'end_date', 'num_days', 'average_duration',\n",
    "                                                   'average_distance', 'total_incentives','average_trip_count' ,'num_operators',\n",
    "                                                   'driver_trip_percentage',\n",
    "                                                   'role_change', 'intraday_change_count',\n",
    "                                                   'total_change_count', 'intraday_change_percentage',\n",
    "                                                   'total_change_percentage', 'carpool_days',\n",
    "                                                   'carpool_day_list', 'trip_id_list', 'operator_list']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 4\n",
    "\n",
    "Ajout dans la bd des insights par phone trunc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_do_nothing_on_conflict(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['phone_trunc', 'departure_date', 'end_date'])\n",
    "    conn.execute(on_duplicate_key_stmt)\n",
    "    \n",
    "phone_trunc_insights_df.to_sql(\n",
    "    name=\"phone_insights\",\n",
    "    schema=\"fraudcheck\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=insert_or_do_nothing_on_conflict\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 5\n",
    "Appliquer la détection de fraude triangulaire sur les données calculées phone_trunc_insights_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la liste des phone_trunc\n",
    "phone_numbers = phone_trunc_insights_df.phone_trunc.to_list()\n",
    "\n",
    "# filtrer les trajets potentiellement frauduleux\n",
    "potential_fraud_carpool_df = df_carpool[df_carpool['phone_trunc'].isin(phone_numbers)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des insight des phone trunc à chaque trajet\n",
    "potential_fraud_carpool_with_insights_df = potential_fraud_carpool_df.merge(phone_trunc_insights_df,how='left',on='phone_trunc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouper les information par trajets\n",
    "filtered_df_grouped = potential_fraud_carpool_with_insights_df.groupby(['operator_journey_id']).agg({'phone_trunc' : list,\n",
    "                                                                                                     'intraday_change_percentage': list,\n",
    "                                                                                                     'intraday_change_count' : list,\n",
    "                                                                                                     'role_change' : list,\n",
    "                                                                                                     'total_change_percentage' : list})\n",
    "filtered_df_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retirer les trajets ou les paricipany n'ont jamais cha,gé de rôles\n",
    "filtered_df_grouped = filtered_df_grouped[filtered_df_grouped['role_change'].apply(lambda x: x != [False, False])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithme de création de groupe frauduleux \n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between connected phone trunc\n",
    "for _, row in filtered_df_grouped.iterrows():\n",
    "    phone_list = row['phone_trunc']\n",
    "    for i in range(len(phone_list) - 1):\n",
    "        for j in range(i + 1, len(phone_list)):\n",
    "            if G.has_edge(phone_list[i], phone_list[j]):\n",
    "                G[phone_list[i]][phone_list[j]]['interactions'] += 1\n",
    "            else:\n",
    "                G.add_edge(phone_list[i], phone_list[j], interactions=1)\n",
    "\n",
    "# Find connected components in the graph\n",
    "#connected_components = [component for component in nx.connected_components(G) if len(component) > 2]\n",
    "connected_components = nx.connected_components(G)\n",
    "\n",
    "# Create DataFrame with groups\n",
    "group_data = []\n",
    "group_degree_centrality = []\n",
    "group_betweenness_centrality = []\n",
    "\n",
    "for idx, component in enumerate(connected_components):\n",
    "    group_graph = G.subgraph(component)\n",
    "    degree_centrality = nx.degree_centrality(group_graph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(group_graph)\n",
    "    group_phones = list(component)\n",
    "    group_journeys = df_carpool[df_carpool['phone_trunc'].isin(group_phones)]\n",
    "    group_journeys = group_journeys.drop_duplicates('operator_journey_id').copy()\n",
    "    group_journeys = potential_fraud_carpool_with_insights_df[potential_fraud_carpool_with_insights_df['phone_trunc'].isin(group_phones)].copy()\n",
    "    group_duration = np.round(group_journeys['duration'].mean())\n",
    "    group_operator_id = group_journeys['operator_journey_id'].copy()\n",
    "    group_journeys['date'] = group_journeys['datetime'].dt.date.copy()\n",
    "    grouped = group_journeys.groupby('phone_trunc').size().reset_index(name='count')\n",
    "    total_change_percentage = np.unique(group_journeys['total_change_percentage'].to_list())\n",
    "    \n",
    "    group_data.append({\n",
    "        'group': idx+1,\n",
    "        'phone_trunc': group_phones,\n",
    "        'num_participants': len(group_phones),\n",
    "        'num_trips': len(group_journeys.operator_journey_id.unique()),\n",
    "        'operator_list' : group_journeys.operator_id.unique(),\n",
    "        'num_operators' : len(group_journeys.operator_id.unique()),\n",
    "        'average_duration': group_duration,\n",
    "        'departure_date': group_journeys['datetime'].min().date(),\n",
    "        'end_date': group_journeys['datetime'].max().date(),\n",
    "        'average_daily_trips' : np.round(group_journeys.drop_duplicates('operator_journey_id')['datetime'].dt.date.value_counts().sort_index().mean()),\n",
    "        #'daily_mean_trips' : grouped.loc[grouped['count'].idxmax(),'count']/len(group_journeys.groupby('date')),\n",
    "        'total_change_percentage' : total_change_percentage,\n",
    "        'total_incentives' : np.sum(group_journeys.drop_duplicates('operator_journey_id')['incentive']*100),\n",
    "        'central_participants' : degree_centrality,\n",
    "        'intermediate_participants' : betweenness_centrality,\n",
    "        'journey_id_list' : group_operator_id,\n",
    "      \n",
    "    })\n",
    "groups_df = pd.DataFrame(group_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 6\n",
    "\n",
    "Ajout dans la bd les groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def insert_or_do_nothing_on_conflict(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['id', 'group', 'phone_trunc'])\n",
    "    conn.execute(on_duplicate_key_stmt)\n",
    "\n",
    "groups_df.to_sql(\n",
    "    name=\"potential_triangular_patterns\",\n",
    "    schema=\"fraudcheck\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=insert_or_do_nothing_on_conflict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
