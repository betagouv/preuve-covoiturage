{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création et Stockage des Tables d'Insights et Triangulaires pour Analyse de Fraude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "# Parameters\n",
    "\n",
    "\n",
    "   Parameter name          Example Value                                            Description\n",
    "- `connection_string` : 'postgresql://postgres:postgres@localhost:5432/local'   -> Postgresql URL connection string\n",
    "- `aom_insee` :          '217500016'                                            -> Aom insee code representing geo perimeter to apply the algorithm\n",
    "- `start_date` :         '2023-02-28 23:59:59'                                  -> Start date\n",
    "- `end_date`:             '2023-04-30 00:00:01'                                 -> End date\n",
    "- `policy_id`             : 459                                                 -> Policy id filter on incentive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "load_dotenv()\n",
    "\n",
    "connection_string = os.getenv('PG_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to french timezone \n",
    "\n",
    "def convert_to_france_time(dt_obj):\n",
    "    original_format = '%Y-%m-%d %H:%M:%S.%f %z'\n",
    "    utc_tz = pytz.timezone('UTC')\n",
    "    france_tz = pytz.timezone('Europe/Paris')\n",
    "    dt_str = dt_obj.strftime(original_format)\n",
    "    dt = datetime.strptime(dt_str, original_format).replace(tzinfo=utc_tz)\n",
    "    return dt.astimezone(france_tz)\n",
    "\n",
    "# Add flag for night trips\n",
    "def is_night_time(time, start, end):\n",
    "    if start <= end:\n",
    "        return start <= time <= end\n",
    "    else:\n",
    "        return start <= time or time <= end\n",
    "  \n",
    "# Calculate percentage of boolean column\n",
    "def calculate_percentages(df,column):\n",
    "    counts = df[column].value_counts(normalize=True) * 100\n",
    "    return counts\n",
    "\n",
    "def intra_day_change_count(row):\n",
    "    if len(row['roles']) <= 1:\n",
    "        return 0\n",
    "    count = sum((row['roles'][i] != row['roles'][i+1]) for i in range(len(row['roles']) - 1) if row['carpool_day_list'][i] == row['carpool_day_list'][i+1])\n",
    "    return count\n",
    "\n",
    "def total_change_count(row):\n",
    "    if len(row['roles']) <= 1:\n",
    "        return 0\n",
    "    count = sum((row['roles'][i] != row['roles'][i+1]) for i in range(len(row['roles']) - 1))\n",
    "    return count\n",
    "\n",
    "def intra_day_change_percentage(row):\n",
    "    unique_days = np.unique(row['carpool_day_list'])\n",
    "    count = sum(1 for day in unique_days if any((row['roles'][i] != row['roles'][i+1]) for i in range(len(row['roles']) - 1) if row['carpool_day_list'][i] == day and row['carpool_day_list'][i+1] == day))\n",
    "    percentage = np.round(count / len(unique_days) * 100, 2)\n",
    "    return percentage\n",
    "\n",
    "def count_consecutive_changes(date_list, role_list):\n",
    "    df = pd.DataFrame({'Date': date_list, 'Role': role_list})\n",
    "    consecutive_changes = []\n",
    "    for date, group in df.groupby('Date'):\n",
    "        consecutive_changes_count = 0\n",
    "        previous_value = None\n",
    "\n",
    "        for index, row in group.iterrows():\n",
    "            current_value = row['Role']\n",
    "\n",
    "            if previous_value is not None and current_value != previous_value:\n",
    "                consecutive_changes_count += 1\n",
    "\n",
    "            previous_value = current_value\n",
    "\n",
    "        consecutive_changes.append(consecutive_changes_count)\n",
    "\n",
    "    return consecutive_changes\n",
    "\n",
    "def check_presence(phone, level_set):\n",
    "    return True if phone in level_set else False\n",
    "\n",
    "def calculate_total_incentive(row, df_carpool):\n",
    "    phone_truncs = row['phone_trunc']\n",
    "    total_incentive = df_carpool[df_carpool.phone_trunc.isin(phone_truncs)].incentive.sum()\n",
    "    return total_incentive\n",
    "\n",
    "def is_changed(current, previous):\n",
    "      if pd.isna(current) and pd.isna(previous):\n",
    "          return False\n",
    "      if pd.isna(current) or pd.isna(previous):\n",
    "          return False\n",
    "      return current != previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_insights_and_triangular_df(start_date, end_date, aom_insee,engine):\n",
    "  \n",
    "  query = f\"\"\"SELECT cc._id, cc.is_driver, ci.phone_trunc, cc.datetime, cc.duration, cc.operator_id, cc.seats,\n",
    "  ST_AsText(cc.start_position) as start_wkt, ST_AsText(cc.end_position) as end_wkt, \n",
    "  cc.operator_journey_id,\n",
    "  cc.distance,\n",
    "  ci.operator_user_id,\n",
    "  cc.end_position,\n",
    "  CASE WHEN pi.amount >= 0 THEN pi.amount ELSE 0 END as incentive,\n",
    "  cc.operator_trip_id,\n",
    "  cc2.is_driver as other_is_driver,\n",
    "  ci2.phone_trunc as other_phone_trunc\n",
    "  FROM CARPOOL.CARPOOLS cc\n",
    "    join carpool.identities ci on cc.identity_id = ci._id\n",
    "    join geo.perimeters gps on cc.start_geo_code = gps.arr and gps.year = 2022\n",
    "    join geo.perimeters gpe on cc.end_geo_code = gpe.arr and gpe.year = 2022\n",
    "    LEFT JOIN policy.incentives pi on pi.carpool_id = cc._id\n",
    "    JOIN CARPOOL.CARPOOLS AS CC2 ON CC.OPERATOR_JOURNEY_ID = CC2.OPERATOR_JOURNEY_ID and CC.is_driver != cc2.is_driver\n",
    "    JOIN CARPOOL.IDENTITIES AS CI2 on CC2.IDENTITY_ID = CI2._id\n",
    "      WHERE CC.DATETIME >= '{start_date}'::timestamp AT TIME ZONE 'EUROPE/PARIS'\n",
    "      AND CC.DATETIME < '{end_date}'::timestamp AT TIME ZONE 'EUROPE/PARIS'\n",
    "      {f\"and (gps.aom = '{aom_insee}' or gpe.aom = '{aom_insee}' or gps.reg = '{aom_insee}' or gpe.reg = '{aom_insee}') and gps.year = 2022 and gpe.year = 2022\" if aom_insee else \"\"}\n",
    "  \"\"\"\n",
    "\n",
    "  with engine.connect() as conn:\n",
    "      df_carpool = pd.read_sql_query(text(query), conn)\n",
    "\n",
    "  \n",
    "  # convert to french datetime\n",
    "  df_carpool['datetime_france'] = df_carpool['datetime'].apply(convert_to_france_time)\n",
    "\n",
    "  df_carpool['day_month_year'] = df_carpool['datetime_france'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "  # add boolean flags for night trips\n",
    "  df_carpool['night_21_to_6'] = df_carpool['datetime_france'].apply(lambda x: is_night_time(x.time(), pd.Timestamp('21:00').time(), pd.Timestamp('06:00').time()))\n",
    "  df_carpool['night_21_to_5'] = df_carpool['datetime_france'].apply(lambda x: is_night_time(x.time(), pd.Timestamp('21:00').time(), pd.Timestamp('05:00').time()))\n",
    "  df_carpool['night_22_to_5'] = df_carpool['datetime_france'].apply(lambda x: is_night_time(x.time(), pd.Timestamp('22:00').time(), pd.Timestamp('05:00').time()))\n",
    "\n",
    "  # Convert and create some features\n",
    "  df_carpool['day'] = df_carpool['datetime_france'].dt.date\n",
    "  df_carpool['incentive'] = df_carpool['incentive']/100\n",
    "  df_carpool['duration'] = np.round(df_carpool['duration']/60)\n",
    "  df_carpool['distance'] = np.round(df_carpool['distance']/1000,1)\n",
    "\n",
    "  # Update the correct number of seats used per journey id\n",
    "  _temp = pd.DataFrame(df_carpool.groupby('operator_journey_id').seats.sum()).reset_index()\n",
    "\n",
    "  _temp.columns = ['operator_journey_id','seats']\n",
    "  df_carpool.drop('seats',axis=1,inplace=True)\n",
    "  df_carpool = df_carpool.merge(_temp,how='left',on='operator_journey_id').copy()\n",
    "\n",
    "  # Creation of insights for each operator user id\n",
    "  phone_trunc_insights_df = df_carpool.groupby('operator_user_id').agg({\n",
    "    'phone_trunc': ['unique'],\n",
    "    'datetime_france': ['min', 'max', list],\n",
    "    'day_month_year' : [list],\n",
    "    'duration':  ['mean', 'count'],\n",
    "    'distance': 'mean',\n",
    "    'incentive': 'sum',\n",
    "    'is_driver': ['mean',list],\n",
    "    'day': ['nunique'],\n",
    "    'operator_journey_id': [list],\n",
    "    'operator_id': [list],\n",
    "    'seats' : ['mean'],\n",
    "    'night_21_to_6' : ['sum', lambda x: any(x), lambda x: x.mean()] ,\n",
    "    'night_21_to_5' : ['sum', lambda x: any(x), lambda x: x.mean()],\n",
    "    'night_22_to_5' : ['sum', lambda x: any(x), lambda x: x.mean()]}) \n",
    "  phone_trunc_insights_df.reset_index(inplace=True)\n",
    "  phone_trunc_insights_df.columns = ['operator_user_id',\n",
    "                                    'phone_trunc',\n",
    "                                    'departure_date',\n",
    "                                    'end_date',\n",
    "                                    'carpool_datetime_list',\n",
    "                                    'carpool_day_list',\n",
    "                                    'average_duration',\n",
    "                                    'num_trips',\n",
    "                                    'average_distance',\n",
    "                                    'total_incentives',\n",
    "                                    'driver_trip_percentage',\n",
    "                                    'roles',\n",
    "                                    'carpool_days',\n",
    "                                    'trip_id_list',\n",
    "                                    'operator_list',\n",
    "                                    'average_seats',\n",
    "                                    'night_time_count_21_6',\n",
    "                                    'has_night_time_21_6',\n",
    "                                    'night_time_percentage_21_6',\n",
    "                                    'night_time_count_21_5',\n",
    "                                    'has_night_time_21_5',\n",
    "                                    'night_time_percentage_21_5',\n",
    "                                    'night_time_count_22_5',\n",
    "                                    'has_night_time_22_5',\n",
    "                                    'night_time_percentage_22_5']\n",
    "\n",
    "  phone_trunc_insights_df['average_duration'] = np.round(phone_trunc_insights_df['average_duration'])\n",
    "  phone_trunc_insights_df['average_distance'] = np.round(phone_trunc_insights_df['average_distance'])\n",
    "\n",
    "  phone_trunc_insights_df['carpool_day_list'] = pd.Series(phone_trunc_insights_df['carpool_day_list'])\n",
    "  phone_trunc_insights_df['trip_id_list'] = pd.Series(phone_trunc_insights_df['trip_id_list'])\n",
    "  phone_trunc_insights_df['operator_list'] = pd.Series(phone_trunc_insights_df['operator_list'])\n",
    "\n",
    "\n",
    "\n",
    "  phone_trunc_insights_df['num_days'] = (phone_trunc_insights_df['end_date'] - phone_trunc_insights_df['departure_date']).dt.days\n",
    "  phone_trunc_insights_df['average_trip_count'] = phone_trunc_insights_df.apply(\n",
    "      lambda row: np.round(row['num_trips'] / row['carpool_days'] if row['carpool_days'] > 0 else 0,1),\n",
    "      axis=1)\n",
    "  phone_trunc_insights_df['driver_trip_percentage'] = np.round(phone_trunc_insights_df['driver_trip_percentage'] * 100,2)\n",
    "\n",
    "\n",
    "  phone_trunc_insights_df['num_operators'] = phone_trunc_insights_df['operator_list'].apply(lambda row: len(np.unique(row)))\n",
    "  phone_trunc_insights_df['role_change'] = phone_trunc_insights_df['roles'].apply(lambda x: len(np.unique(x)) > 1)\n",
    "  phone_trunc_insights_df['intraday_change_count'] = phone_trunc_insights_df.apply(lambda row: math.ceil(np.mean(count_consecutive_changes(row['carpool_day_list'], row['roles']))), axis=1)\n",
    "  phone_trunc_insights_df['total_change_count'] = phone_trunc_insights_df.apply(total_change_count, axis=1)\n",
    "  phone_trunc_insights_df['intraday_change_percentage'] = phone_trunc_insights_df.apply(intra_day_change_percentage, axis=1)\n",
    "  phone_trunc_insights_df['total_change_percentage'] = phone_trunc_insights_df.apply(lambda row: np.round(row['total_change_count'] / len(row['carpool_day_list']) * 100, 2), axis=1)\n",
    "  phone_trunc_insights_df = phone_trunc_insights_df[['phone_trunc','operator_user_id', 'departure_date', 'end_date', 'num_days', 'average_duration',\n",
    "                                                    'average_distance', 'total_incentives','average_trip_count' ,'num_operators',\n",
    "                                                    'driver_trip_percentage',\n",
    "                                                    'role_change', 'intraday_change_count',\n",
    "                                                    'total_change_count', 'intraday_change_percentage',\n",
    "                                                    'total_change_percentage', 'carpool_days',\n",
    "                                                    'carpool_day_list', 'trip_id_list', 'operator_list','average_seats',\n",
    "                                                    'night_time_count_21_6','has_night_time_21_6','night_time_percentage_21_6',\n",
    "                                                    'night_time_count_21_5','has_night_time_21_5','night_time_percentage_21_5',\n",
    "                                                    'night_time_count_22_5','has_night_time_22_5','night_time_percentage_22_5']]\n",
    "  phone_trunc_insights_df['operator_list'] = phone_trunc_insights_df['operator_list'].tolist()\n",
    "  phone_trunc_insights_df['phone_trunc'] = phone_trunc_insights_df['phone_trunc'].astype(str)\n",
    "  phone_trunc_insights_df['phone_trunc'] = phone_trunc_insights_df['phone_trunc'].str.replace('[', '', regex=False).str.replace(']', '', regex=False)\n",
    "  phone_trunc_insights_df['phone_trunc'] = phone_trunc_insights_df['phone_trunc'].str.replace(\"'\", \"\")\n",
    "  phone_trunc_insights_df['night_time_percentage_21_5'] = np.round(100*phone_trunc_insights_df.night_time_percentage_21_5,2)\n",
    "  phone_trunc_insights_df['night_time_percentage_21_6'] = np.round(100*phone_trunc_insights_df.night_time_percentage_21_6,2)\n",
    "  phone_trunc_insights_df['night_time_percentage_22_5'] = np.round(100*phone_trunc_insights_df.night_time_percentage_22_5,2)\n",
    "  mean_seats = phone_trunc_insights_df.average_seats.mean()\n",
    "  phone_trunc_insights_df['occupancy_rate_exceeded'] = phone_trunc_insights_df.average_seats > mean_seats\n",
    "\n",
    "  # Triangular Table Creation\n",
    "  phone_numbers = phone_trunc_insights_df.phone_trunc.to_list()\n",
    "  potential_fraud_carpool_df = df_carpool[df_carpool['phone_trunc'].isin(phone_numbers)].copy()\n",
    "  potential_fraud_carpool_with_insights_df = potential_fraud_carpool_df.merge(phone_trunc_insights_df,how='left',on='phone_trunc')\n",
    "\n",
    "  potential_fraud_carpool_with_insights_df_level_1 = potential_fraud_carpool_with_insights_df[potential_fraud_carpool_with_insights_df.total_change_count / potential_fraud_carpool_with_insights_df.carpool_days >= 2].copy()\n",
    "  potential_fraud_carpool_with_insights_df_level_2 = potential_fraud_carpool_with_insights_df[potential_fraud_carpool_with_insights_df.total_change_count / potential_fraud_carpool_with_insights_df.carpool_days >= 1].copy()\n",
    "  # For level 1 \n",
    "  filtered_df_grouped_level_1 = potential_fraud_carpool_with_insights_df_level_1.groupby(['operator_journey_id']).agg({'phone_trunc' : list,\n",
    "                                                                                                      'intraday_change_percentage': list,\n",
    "                                                                                                      'intraday_change_count' : list,\n",
    "                                                                                                      'role_change' : list,\n",
    "                                                                                                      'total_change_percentage' : list})\n",
    "  filtered_df_grouped_level_1.reset_index(inplace=True)\n",
    "  filtered_df_grouped_level_1.columns  = ['operator_journey_id', 'phone_trunc','intraday_change_percentage','intraday_change_count', 'role_change', 'total_change_percentage']\n",
    "  filtered_df_grouped_level_1 = filtered_df_grouped_level_1[filtered_df_grouped_level_1['role_change'].apply(lambda x: x != [False, False])].copy()\n",
    "\n",
    "\n",
    "  # For level 2\n",
    "  filtered_df_grouped_level_2 = potential_fraud_carpool_with_insights_df_level_2.groupby(['operator_journey_id']).agg({'phone_trunc' : list,\n",
    "                                                                                                      'intraday_change_percentage': list,\n",
    "                                                                                                      'intraday_change_count' : list,\n",
    "                                                                                                      'role_change' : list,\n",
    "                                                                                                      'total_change_percentage' : list})\n",
    "  filtered_df_grouped_level_2.reset_index(inplace=True)\n",
    "  filtered_df_grouped_level_2.columns  = ['operator_journey_id', 'phone_trunc','intraday_change_percentage','intraday_change_count', 'role_change', 'total_change_percentage']\n",
    "  filtered_df_grouped_level_2 = filtered_df_grouped_level_1[filtered_df_grouped_level_2['role_change'].apply(lambda x: x != [False, False])].copy()\n",
    "\n",
    "  journey_to_phones = potential_fraud_carpool_with_insights_df_level_2.groupby('operator_journey_id')['phone_trunc'].apply(list).to_dict()\n",
    "\n",
    "  # algorithme de création de groupe frauduleux \n",
    "  G = nx.Graph()\n",
    "  journey_to_phones = potential_fraud_carpool_with_insights_df_level_1.groupby('operator_journey_id')['phone_trunc'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "  for journey, phones in journey_to_phones.items():\n",
    "      # Ensure all phone numbers are unique in the list to avoid self-loops\n",
    "      unique_phones = set(phones)\n",
    "      # Create edges between all pairs of phone_truncs for this journey\n",
    "      for phone1 in unique_phones:\n",
    "          for phone2 in unique_phones:\n",
    "              if phone1 != phone2:\n",
    "                  if G.has_edge(phone1, phone2):\n",
    "                      G[phone1][phone2]['shared_journeys'].add(journey)\n",
    "                  else:\n",
    "                      G.add_edge(phone1, phone2, shared_journeys={journey})\n",
    "\n",
    "\n",
    "  connected_components = nx.connected_components(G)\n",
    "\n",
    "  group_data = []\n",
    "\n",
    "  for idx, component in enumerate(connected_components):\n",
    "      group_graph = G.subgraph(component)\n",
    "      degree_centrality = nx.degree_centrality(group_graph)\n",
    "      betweenness_centrality = nx.betweenness_centrality(group_graph)\n",
    "      group_phones = pd.Series(list(component))\n",
    "      group_journeys = df_carpool[df_carpool['phone_trunc'].isin(group_phones)]\n",
    "      group_journeys = group_journeys.drop_duplicates('operator_journey_id').copy()\n",
    "      group_journeys = potential_fraud_carpool_with_insights_df[potential_fraud_carpool_with_insights_df['phone_trunc'].isin(group_phones)].copy()\n",
    "      group_duration = np.round(group_journeys['duration'].mean())\n",
    "      group_operator_id = group_journeys['operator_journey_id'].copy()\n",
    "      group_journeys['date'] = group_journeys['datetime'].dt.date.copy()\n",
    "      total_change_percentage = np.unique(group_journeys['total_change_percentage'].to_list())\n",
    "    \n",
    "      group_data.append({\n",
    "          'groupe': idx+1,\n",
    "          'phone_trunc': list(group_phones),\n",
    "          'num_participants': len(group_phones),\n",
    "          'num_trips': len(group_journeys.operator_journey_id.unique()),\n",
    "          'operator_list' : list(group_journeys.operator_id.unique()),\n",
    "          'num_operators' : len(group_journeys.operator_id.unique()),\n",
    "          'average_duration': group_duration,\n",
    "          'departure_date': group_journeys['datetime'].min(),\n",
    "          'end_date': group_journeys['datetime'].max(),\n",
    "          'average_daily_trips' : np.round(group_journeys.drop_duplicates('operator_journey_id')['datetime'].dt.date.value_counts().sort_index().mean()),\n",
    "          'total_change_percentage' : list(total_change_percentage),\n",
    "          'total_incentives' : np.sum(group_journeys.drop_duplicates('operator_journey_id')['incentive']),\n",
    "          'central_participants' : degree_centrality,\n",
    "          'intermediate_participants' : betweenness_centrality,\n",
    "          'journey_id_list' : list(group_operator_id),\n",
    "      })\n",
    "  groups_df_level_1 = pd.DataFrame(group_data)\n",
    "\n",
    "  groups_df_level_1['operator_list'] = groups_df_level_1['operator_list'].apply(lambda x: list(map(int, x)))\n",
    "  groups_df_level_1['total_change_percentage'] = groups_df_level_1['total_change_percentage'].apply(lambda x: list(map(float, x)))\n",
    "  groups_df_level_1['central_participants'] = groups_df_level_1['central_participants'].apply(json.dumps)\n",
    "  groups_df_level_1['intermediate_participants'] = groups_df_level_1['intermediate_participants'].apply(json.dumps)\n",
    "  groups_df_level_1['central_participants'] = pd.Series(groups_df_level_1['central_participants'])\n",
    "  groups_df_level_1['intermediate_participants'] = pd.Series(groups_df_level_1['intermediate_participants'])\n",
    "\n",
    "\n",
    "  # Level 2\n",
    "  G = nx.Graph()\n",
    "  journey_to_phones = potential_fraud_carpool_with_insights_df_level_2.groupby('operator_journey_id')['phone_trunc'].apply(list).to_dict()\n",
    "\n",
    "  for journey, phones in journey_to_phones.items():\n",
    "      unique_phones = set(phones)\n",
    "      for phone1 in unique_phones:\n",
    "          for phone2 in unique_phones:\n",
    "              if phone1 != phone2:\n",
    "                  if G.has_edge(phone1, phone2):\n",
    "                      G[phone1][phone2]['shared_journeys'].add(journey)\n",
    "                  else:\n",
    "                      G.add_edge(phone1, phone2, shared_journeys={journey})\n",
    "\n",
    "\n",
    "  connected_components = nx.connected_components(G)\n",
    "\n",
    "  group_data = []\n",
    "\n",
    "  for idx, component in enumerate(connected_components):\n",
    "      group_graph = G.subgraph(component)\n",
    "      degree_centrality = nx.degree_centrality(group_graph)\n",
    "      betweenness_centrality = nx.betweenness_centrality(group_graph)\n",
    "      group_phones = pd.Series(list(component))\n",
    "      group_journeys = df_carpool[df_carpool['phone_trunc'].isin(group_phones)]\n",
    "      group_journeys = group_journeys.drop_duplicates('operator_journey_id').copy()\n",
    "      group_journeys = potential_fraud_carpool_with_insights_df[potential_fraud_carpool_with_insights_df['phone_trunc'].isin(group_phones)].copy()\n",
    "      group_duration = np.round(group_journeys['duration'].mean())\n",
    "      group_operator_id = group_journeys['operator_journey_id'].copy()\n",
    "      group_journeys['date'] = group_journeys['datetime'].dt.date.copy()\n",
    "      total_change_percentage = np.unique(group_journeys['total_change_percentage'].to_list())\n",
    "      \n",
    "      group_data.append({\n",
    "          'groupe': idx+1,\n",
    "          'phone_trunc': list(group_phones),\n",
    "          'num_participants': len(group_phones),\n",
    "          'num_trips': len(group_journeys.operator_journey_id.unique()),\n",
    "          'operator_list' : list(group_journeys.operator_id.unique()),\n",
    "          'num_operators' : len(group_journeys.operator_id.unique()),\n",
    "          'average_duration': group_duration,\n",
    "          'departure_date': group_journeys['datetime'].min(),\n",
    "          'end_date': group_journeys['datetime'].max(),\n",
    "          'average_daily_trips' : np.round(group_journeys.drop_duplicates('operator_journey_id')['datetime'].dt.date.value_counts().sort_index().mean()),\n",
    "          'total_change_percentage' : list(total_change_percentage),\n",
    "          'total_incentives' : np.sum(group_journeys.drop_duplicates('operator_journey_id')['incentive']),\n",
    "          'central_participants' : degree_centrality,\n",
    "          'intermediate_participants' : betweenness_centrality,\n",
    "          'journey_id_list' : list(group_operator_id),\n",
    "      })\n",
    "  groups_df_level_2 = pd.DataFrame(group_data)\n",
    "\n",
    "  groups_df_level_1['level'] = 1\n",
    "  groups_df_level_2['level'] = 2\n",
    "\n",
    "  groups_df_combined = pd.concat([groups_df_level_1, groups_df_level_2], ignore_index=True)\n",
    "\n",
    "  groups_df_level_2['operator_list'] = groups_df_level_2['operator_list'].apply(lambda x: list(map(int, x)))\n",
    "  groups_df_level_2['total_change_percentage'] = groups_df_level_2['total_change_percentage'].apply(lambda x: list(map(float, x)))\n",
    "  groups_df_level_2['central_participants'] = groups_df_level_2['central_participants'].apply(json.dumps)\n",
    "  groups_df_level_2['intermediate_participants'] = groups_df_level_2['intermediate_participants'].apply(json.dumps)\n",
    "  groups_df_level_2['central_participants'] = pd.Series(groups_df_level_2['central_participants'])\n",
    "  groups_df_level_2['intermediate_participants'] = pd.Series(groups_df_level_2['intermediate_participants'])\n",
    "\n",
    "  # Add insights to the phone_trunc_insights_df\n",
    "  phones_level_1 = set(phone for sublist in groups_df_level_1['phone_trunc'] for phone in sublist)\n",
    "  phones_level_2 = set(phone for sublist in groups_df_level_2['phone_trunc'] for phone in sublist)\n",
    "  phone_trunc_insights_df['triangular_level_1'] = phone_trunc_insights_df['phone_trunc'].apply(lambda x: check_presence(x, phones_level_1))\n",
    "  phone_trunc_insights_df['triangular_level_2'] = phone_trunc_insights_df['phone_trunc'].apply(lambda x: check_presence(x, phones_level_2))\n",
    "\n",
    "\n",
    "  journey_ids_level_1 = potential_fraud_carpool_with_insights_df_level_1.operator_journey_id.unique()\n",
    "  journey_ids_level_2 = potential_fraud_carpool_with_insights_df_level_2.operator_journey_id.unique()\n",
    "  phones_in_flagged_journeys_level_1 = df_carpool[df_carpool['operator_journey_id'].isin(journey_ids_level_1)]['phone_trunc'].unique()\n",
    "  phones_in_flagged_journeys_level_2 = df_carpool[df_carpool['operator_journey_id'].isin(journey_ids_level_2)]['phone_trunc'].unique()\n",
    "  phone_trunc_insights_df['traveled_with_level_1'] = phone_trunc_insights_df['phone_trunc'].isin(phones_in_flagged_journeys_level_1)\n",
    "  phone_trunc_insights_df['traveled_with_level_2'] = phone_trunc_insights_df['phone_trunc'].isin(phones_in_flagged_journeys_level_2)\n",
    "\n",
    "  phone_trunc_insights_df.drop_duplicates(subset='phone_trunc',inplace=True)\n",
    "\n",
    "  # Merge triangulars to avoid redundancy\n",
    "  groups_df_combined['phone_trunc_set'] = groups_df_combined['phone_trunc'].apply(set)\n",
    "  groups_df_combined = groups_df_combined[['phone_trunc', 'num_participants', 'num_trips',\n",
    "        'operator_list', 'num_operators', 'average_duration', 'departure_date',\n",
    "        'end_date', 'average_daily_trips', 'total_change_percentage',\n",
    "        'total_incentives','journey_id_list', 'level','phone_trunc_set']].copy()\n",
    "  \n",
    "\n",
    "  G = nx.Graph()\n",
    "  for (idx1, row1), (idx2, row2) in itertools.combinations(groups_df_combined.iterrows(), 2):\n",
    "      if row1['phone_trunc_set'] & row2['phone_trunc_set']:  # If intersection is not empty\n",
    "          G.add_edge(idx1, idx2)\n",
    "  components = list(nx.connected_components(G))\n",
    "  merged_rows = []\n",
    "\n",
    "  for component in components:\n",
    "      component_indices = list(component)\n",
    "      rows = groups_df_combined.loc[component_indices]\n",
    "      merged_row = {\n",
    "          'phone_trunc': list(set(itertools.chain.from_iterable(rows['phone_trunc']))),\n",
    "          'num_participants': None,\n",
    "          'total_incentives' : rows['total_incentives'].sum(),\n",
    "          'num_trips': rows['num_trips'].sum(),\n",
    "          'operator_list': list(set(itertools.chain.from_iterable(rows['operator_list']))),\n",
    "          'num_operators': None,  \n",
    "          'average_duration': rows['average_duration'].mean(),\n",
    "          'departure_date': rows['departure_date'].min(),\n",
    "          'end_date': rows['end_date'].max(),\n",
    "          'average_daily_trips': rows['average_daily_trips'].mean(),\n",
    "          'level': 1 if any(rows['level'] == 1) else 2 \n",
    "      }\n",
    "\n",
    "      merged_row['num_participants'] = len(merged_row['phone_trunc'])\n",
    "      merged_row['num_operators'] = len(merged_row['operator_list'])\n",
    "\n",
    "      merged_rows.append(merged_row)\n",
    "\n",
    "  merged_df = pd.DataFrame(merged_rows)\n",
    "  all_indices = set(groups_df_combined.index)\n",
    "  merged_indices = set(itertools.chain.from_iterable(components))\n",
    "  unmerged_indices = all_indices - merged_indices\n",
    "  unmerged_indices_list = list(unmerged_indices)\n",
    "  unmerged_df = groups_df_combined.loc[unmerged_indices_list]\n",
    "  final_triangular_df = pd.concat([merged_df, unmerged_df[merged_df.columns]], ignore_index=True)\n",
    "  final_triangular_df['total_incentives'] = final_triangular_df.apply(calculate_total_incentive, df_carpool=df_carpool, axis=1)\n",
    "\n",
    "  # Identity phone_trunc changes\n",
    "  operator_user_ids = df_carpool.operator_user_id.unique().tolist()\n",
    "  formatted_ids = ', '.join(f\"'{id}'\" for id in operator_user_ids)\n",
    "  query = f\"\"\"\n",
    "  SELECT \n",
    "      operator_user_id, \n",
    "      phone_trunc,\n",
    "      identity_key, \n",
    "      created_at, \n",
    "      updated_at\n",
    "  FROM \n",
    "      carpool.identities\n",
    "  WHERE \n",
    "      operator_user_id IN ({formatted_ids}) AND updated_at < '{end_date}'::timestamp AT TIME ZONE 'EUROPE/PARIS';\n",
    "  \"\"\"\n",
    "\n",
    "  with engine.connect() as conn:\n",
    "    df_identities = pd.read_sql_query(text(query), conn)\n",
    "\n",
    "  df_identities['updated_at'] = pd.to_datetime(df_identities['updated_at'])\n",
    "  df_identities.sort_values(by=['operator_user_id', 'updated_at'], inplace=True)\n",
    "  df_identities['year_month'] = df_identities['updated_at'].dt.to_period('M')\n",
    "  df_identities['prev_phone_trunc'] = df_identities.groupby('operator_user_id')['phone_trunc'].shift(1)\n",
    "  df_identities['prev_identity_key'] = df_identities.groupby('operator_user_id')['identity_key'].shift(1)\n",
    "  df_identities['phone_trunc_changed'] = df_identities.apply(lambda row: is_changed(row['phone_trunc'], row['prev_phone_trunc']), axis=1)\n",
    "  df_identities['identity_key_changed'] = df_identities.apply(lambda row: is_changed(row['identity_key'], row['prev_identity_key']), axis=1)\n",
    "  df_identities['change_detected'] = df_identities['phone_trunc_changed'] | df_identities['identity_key_changed']\n",
    "  user_phone_change_history_df = df_identities.groupby('year_month')['identity_key_changed'].sum().reset_index()\n",
    "  user_phone_change_history_df.columns = ['year_month', 'total_changes']\n",
    "\n",
    "  # Add to phone_trunc_insights_df the number of changes per operator_user_id\n",
    "  phone_trunc_insights_df = phone_trunc_insights_df.merge(df_identities.groupby('operator_user_id')['phone_trunc_changed'].sum().reset_index(),how='left').copy()\n",
    "  \n",
    "  return df_carpool,phone_trunc_insights_df,final_triangular_df,user_phone_change_history_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aom_insee = '217500016'\n",
    "start_date ='2024-02-08 23:59:59' \n",
    "end_date='2024-02-10 00:00:01'\n",
    "policy_id = 459\n",
    "engine = create_engine(connection_string, connect_args={'sslmode':'require'})\n",
    "\n",
    "\n",
    "df_carpool,phone_trunc_insights_df,final_triangular_df,user_phone_change_history_df = create_insights_and_triangular_df(start_date, end_date, aom_insee,engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the phone_trunc_insights_df to the db\n",
    "\n",
    "def insert_or_do_nothing_on_conflict(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['phone_trunc', 'departure_date', 'end_date'])\n",
    "    conn.execute(on_duplicate_key_stmt)\n",
    "    \n",
    "phone_trunc_insights_df.to_sql(\n",
    "    name=\"phone_insights_detailed\",\n",
    "    schema=\"fraudcheck\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=insert_or_do_nothing_on_conflict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_triangular_df['operator_list'] = final_triangular_df['operator_list'].apply(lambda x: list(map(int, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the final_triangular_df to the db\n",
    "\n",
    "def insert_or_do_nothing_on_conflict(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['phone_trunc', 'departure_date', 'end_date'])\n",
    "    conn.execute(on_duplicate_key_stmt)\n",
    "    \n",
    "final_triangular_df.to_sql(\n",
    "    name=\"triangular_patterns\", \n",
    "    schema=\"fraudcheck\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=insert_or_do_nothing_on_conflict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_phone_change_history_df['year_month'] = user_phone_change_history_df['year_month'].apply(lambda x: x.to_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store user_phone_change_history_df to the db \n",
    "def insert_or_do_nothing_on_conflict(table, conn, keys, data_iter):\n",
    "    insert_stmt = insert(table.table).values(list(data_iter))\n",
    "    on_duplicate_key_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['year_month'])\n",
    "    conn.execute(on_duplicate_key_stmt)\n",
    "    \n",
    "user_phone_change_history_df.to_sql(\n",
    "    name=\"user_phone_change_history\", \n",
    "    schema=\"fraudcheck\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=insert_or_do_nothing_on_conflict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transactions-categorization-P9hgNzXL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
